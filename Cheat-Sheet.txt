Kubernetes Essentials - 

Lab 1: Kubernetes Operations on AWS
Task 1: Setting up a Kubernetes Cluster

wget files.cloudthat.training/devops/kubernetes-essentials/kc-install-aws-kops-kv1.17.sh
bash kc-install-aws-kops-kv1.17.sh
export KOPS_STATE_STORE=s3://< Your Cluster_Name >
kops validate cluster
sudo hostnamectl set-hostname kops

Task 2: 
Task: Create pod using kubectl command

kubectl run mypod --image=nginx:latest
#list pods
kubectl get pods

#ssh/login to that pod

kubectl exec -it mypod -- bash 
#to comeout of pod run exit command.
exit
# to Read pod's log
kubectl logs mypod
#Describe pod to see it's configuration
kubectl describe pod mypod

Task 3:
# multiple container in pods 

vi multi-container.yaml

apiVersion: v1
kind: Pod
metadata:
  labels:
    app: myapp
  name: multi-con-pod
spec:
  containers:
  - name: con1   ### first container 
    image: nginx:latest
    ports:
    - containerPort: 80
  - name: con2    ## second container 
    image: tomcat
    ports:
    - containerPort: 8080

# to save file :wq

create pod by using below command 
kubectl create -f multi-container.yaml

a. read logs of specific contaienr
 kubectl logs {pod-name} -c {container-name}
 kubectl logs multi-con-pod -c con2

b. login into specific container
kubectl exec -it {pod-name} -c {container-name} -- bash
kubectl exec -it multi-con-pod -c con1 -- bash

to out of container run exit command 


Lab 2: Services in Kubernetes


Task 1: Create a Pod using a YAML file

kubectl get nodes
kubectl get nodes -o wide

vi httpd-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: httpd-pod
  labels:
    app: myapp
spec:
  containers:
  - name: httpd-container
    image: httpd
    ports:
      - containerPort: 80

# to save file use :wq!


kubectl create -f httpd-pod.yaml
kubectl get pods
kubectl get pods -o wide
kubectl get pod httpd-pod -o yaml
kubectl describe pod httpd-pod 
kubectl get pods --show-labels

Task 2: Setup ClusterIP service

vi httpd-svc.yaml

apiVersion: v1
kind: Service
metadata:
  name: httpd-svc
spec:
  selector:
    app: myapp
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  type: ClusterIP

# to save :wq

kubectl apply -f httpd-svc.yaml
kubectl get services
kubectl describe svc httpd-svc
kubectl get ep 
kubectl get svc httpd-svc

Create a pod and login to that pod to execute curl command 

kubectl run testpod --image=httpd:latest --port 80
kubectl exec -it testpod -- bash
apt update && apt install curl -y
curl http://httpd-svc

kubectl get nodes -owide | awk '{print $7}'

ssh -t admin@< replace Node_IP> curl < replace Cluster_IP>:< replace Service_Port>

Task 3: Setup NodePort service

vi httpd-svc.yaml

# Replace the content of httpd-svc.yaml with below content use ESc and :wq! to quit vi

apiVersion: v1
kind: Service
metadata:
  name: httpd-svc
spec:
  selector:
    app: myapp
  ports:
  - protocol: TCP
    port: 80
  type: NodePort

# End of file

kubectl apply -f httpd-svc.yaml

kubectl get svc 
kubectl describe svc httpd-svc
kubectl get nodes -owide | grep node | awk '{print $7}'
curl http://<Your NODE_EXTERNAL_IP>:<Your NodePort>


Task 4: Setup LoadBalancer service

vi httpd-svc.yaml

# Replace the content of httpd-svc.yaml with below content use ESc and :wq! to quit vi

apiVersion: v1
kind: Service
metadata:
  name: httpd-svc
spec:
  selector:
    app: myapp
  ports:
  - protocol: TCP
    port: 80
  type: LoadBalancer


# End of file

kubectl apply -f httpd-svc.yaml
kubectl describe svc httpd-svc
curl http://<Your LoadBalancer_Ingress>:<Your Service_Port>

Task 5: Delete and recreate httpd Pod

kubectl get pod httpd-pod -o wide
kubectl describe svc httpd-svc
kubectl delete pod httpd-pod
kubectl describe svc httpd-svc
kubectl create -f httpd-pod.yaml
kubectl get pod httpd-pod -o wide
kubectl describe svc httpd-svc

Task 6: Clean-up

kubectl delete -f httpd-pod.yaml
kubectl delete -f httpd-svc.yaml

Task: 6 Resource allocation 

vi resource-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  labels:
    app: myapp
  name: res-limit-pod
spec:
  containers:
  - name: con1
    image: nginx:latest
    ports:
    - containerPort: 80
    resources:
      requests:
        memory: "64Mi"
        cpu: "200m"
      limits:
        memory: "128Mi"
        cpu: "500m"
  - name: con2
    image: tomcat
    ports:
    - containerPort: 8080

#to save file :wq
kubectl create -f resource-pod.yaml
kubectl get pod

kubectl describe pod res-limit-pod


******************
Lab 3: Deployment*
******************
Task 1: Write a Deployment YAML and Apply it

wget https://hpe-content.s3.ap-south-1.amazonaws.com/dep-nginx.yaml
kubectl apply -f dep-nginx.yaml
kubectl get deployments
kubectl get rs
kubectl get pods

kubectl exec -it <replace pod_name> -- /bin/bash
nginx -v
exit

Task 2: Update the Deployment with a newer image

kubectl set image deployment/nginx-dep nginx-ctr=nginx:1.11 --record
kubectl describe deployments
kubectl get pods

kubectl exec -it <replace pod_name> -- /bin/bash
nginx -v
exit

Task 3: Rollback of Deployment 

kubectl rollout history deployment/nginx-dep
kubectl rollout undo deployment/nginx-dep --to-revision=1
kubectl get pods

kubectl exec -it <replace pod_name> -- /bin/bash
nginx -v
exit

Task 4: Scaling of Deployments

kubectl get deployments
kubectl get pods

kubectl scale deployment nginx-dep --replicas=8
kubectl get deployments
kubectl get pods

kubectl scale deployment nginx-dep --replicas=2
kubectl get deployments
kubectl get pods

kubectl delete -f dep-nginx.yaml

Task 5: Deployment Auto Scaling HPA - Horizontal Pod Autoscaling

kubectl autoscale deployment nginx-dep --min=3 --max=8 --cpu-percent=70
kubectl get hpa

Lab 4: DaemonSet in Kubernetes

wget https://hpe-content.s3.ap-south-1.amazonaws.com/ds-pod.yaml

kubectl apply -f ds-pod.yaml
kubectl get ds fluent-ds
kubectl get pods -o wide
kubectl delete -f ds-pod.yaml

************
*  Volumes *
************


Lab 5: Empty Dir Volume

vi emp-vol-dep.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: emp-vol-dep
spec:
  replicas: 2
  selector:
    matchLabels:
      app: emp-app
  template:
    metadata:
      labels:
        app: emp-app
    spec:
      containers:
      - name: con1
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: myvol
          mountPath: /data
      - name: con2
        image: tomcat:latest
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: myvol
          mountPath: /data
      volumes:
      - name: myvol
        emptyDir: {}

# save file  Press ESc and :wq! to save and exit vi
kubectl create -f emp-vol-dep.yaml
kubectl get deployment


kubectl exec -it <pod-name> -c con1 -- bash
cd /data
create a  file
touch mango.txt
exit

now login to second container and check you mango file

kubectl exec -it <pod-name> -c con1 -- bash
cd /data
ls


Lab 6: Persistent Volume in Kubernetes using storage-class
Task 1: create pvc persistant volume claim

vi pvc.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 2Gi

# save file  Press ESc and :wq! to save and exit vi
kubectl create -f pvc.yaml


Task 2: Create Pod to use persistent volume claim 

vi pvc-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: mypod
  labels:
     app: myapp
     env: dev
spec:
 containers:
 - name: con1
   image: nginx:latest
   ports:
   - containerPort: 80
   volumeMounts:
   - name: myvol
     mountPath: /mnt
 volumes:
 - name: myvol
   persistentVolumeClaim:
      claimName: myclaim

# Press ESc and :wq! to save and exit vi

kubectl apply -f pvc-pod.yaml
kubectl get pvc
kubectl exec -it mypod -- bash
cd /mnt
create a  file 
touch mango.txt
exit

delete pod 
kubectl delete -f pvc-pod.yaml
re-create pod again
kubectl create -f pvc-pod.yaml
check your mango.txt file is already there
kubectl exec -it mypod -- bash
cd /mnt
ls


Lab 6: StatefulSet Implementation

Task 1:  Creating a StatefulSet

wget files.cloudthat.training/devops/kubernetes-essentials/nginx-sts.yaml

kubectl apply -f nginx-sts.yaml

kubectl get service 
kubectl get statefulset 
kubectl get pods 

for i in 0 1; do kubectl exec "nginx-sts-$i" -- sh -c 'hostname'; done

kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm

nslookup nginx-sts-0.nginx-svc
nslookup nginx-sts-1.nginx-svc

exit

kubectl delete pod -l app=nginx-sts
kubectl get pod  -l app=nginx-sts

for i in 0 1; do kubectl exec "nginx-sts-$i" -- sh -c 'hostname'; done

Task 2: Writing to a Scalable Storage

kubectl get pvc -l app=nginx-sts

for i in 0 1; do kubectl exec "nginx-sts-$i" -- sh -c 'echo "$(hostname)" > /usr/share/nginx/html/index.html'; done

for i in 0 1; do kubectl exec -i -t "nginx-sts-$i" -- curl http://localhost/; done

Task 3: Scaling StatefulSet

kubectl scale sts nginx-sts --replicas=5
kubectl get pods -w -l app=nginx-sts

kubectl get pods -l app=nginx-sts
kubectl get pvc -l app=nginx-sts

kubectl edit sts nginx-sts

# Set spec: replicas to 3, Press Esc and :wq! to exit vi

kubectl get pods -w -l app=nginx-sts
kubectl get pvc -l app=nginx-sts

Task 4: Rolling Update

kubectl edit sts nginx-sts

# Change the spec: conatiners: image value to nginx:latest, Press Esc and :wq! to exit vi

kubectl get pod -l app=nginx-sts 
kubectl get pods
kubectl describe pod nginx-sts-0 | grep Image
kubectl delete -f nginx-sts.yaml

Task 5: Clean-up Resources

kubectl delete -f nginx-sts.yaml



Task 6: Deploy pod in specific node 

root@ip-172-31-30-120:~/allyamls# kubectl get nodes
NAME                                          STATUS   ROLES                  AGE   VERSION
ip-172-20-45-33.us-west-1.compute.internal    Ready    node                   13m   v1.23.16  ## Copy node name and paste in nodeName section in below YAML
ip-172-20-58-92.us-west-1.compute.internal    Ready    control-plane,master   14m   v1.23.16
ip-172-20-60-194.us-west-1.compute.internal   Ready    node                   13m   v1.23.16



vi node-name.yaml

apiVersion: v1
kind: Pod
metadata:
  name: nginx-nodename
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeName: ip-172-20-45-33.us-west-1.compute.internal

kubectl create -f node-name.yaml
kubectl get pods -o wide 
see your pod in defined node



Task 7:  Deploy pod in selected nodes based on node labels.
 
kubectl get nodes --show-labels
assign label to any node
kubectl label node {NODE-NAME}  disktype=ssd

#Find node with your label
kubectl get nodes --show-labels | grep "disktype=ssd"

vi node-label.yaml

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    disktype: ssd

kubectl create -f node-label.yaml
kubectl get pods -o wide

Task 8: Pod affinity

vi depend-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: pa-nginx-pod1
  labels:
    app: pa-nginx-app
spec:
  containers:
  - name: pa-nginx-ctr1
    image: nginx

kubectl create -f depend-pod.yaml
kubectl get pods


Task 9: pod affinity 

vi pod-affinity.yaml

apiVersion: v1
kind: Pod
metadata:
  name: pa-nginx-pod2
spec:
  containers:
  - name: pa-nginx-ctr2
    image: nginx
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - pa-nginx-app
        topologyKey: kubernetes.io/hostname

kubectl create -f pod-affinity.yaml

kubectl get pods -o wide
both pod are in same node




Task 9: Pod Anti Affinity

vi pod-antiaff.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - pa-nginx-app
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: nginx
        image: nginx

kubectl create -f  pod-antiaff.yaml
kubectl get pod -o wide 


kubectl delete -f  pod-antiaff.yaml



Task 10: Node Affinity  
to check lables:
kubectl get nodes --show-labels | grep "disktype"

add label on worker node
kubectl label nodes {node-name} disktype=ssd

vi node-aff-pref.yaml

apiVersion: v1
kind: Pod
metadata:
  name: na-nginx-pod1
spec:
  containers:
  - name: na-nginx-ctr1
    image: nginx
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd

kubectl create -f  node-aff-pref.yaml.yaml
kubectl get pod -o wide

 remove label from node
 kubectl label nodes ip-172-20-33-243.us-west-1.compute.internal disktype-
 and see your pod will deploy in any node


kubectl delete -f  node-aff-pref.yaml

###
now below required node affinity pod and you will be seeing your pod is in pending state
after that add following label in  any worker node after labeling nodeyour pod will deploy 
in that node only.

kubectl label nodes ip-172-20-33-243.us-west-1.compute.internal disktype=ssd

vi node-aff-req.yaml

apiVersion: v1
kind: Pod
metadata:
  name: na-nginx-pod2
spec:
  containers:
  - name: na-nginx-ctr2
    image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd

kubectl create -f  node-aff-req.yaml
kubectl get pod -o wide

remove label from node
 kubectl label nodes ip-172-20-33-243.us-west-1.compute.internal disktype-

after redeploy pod and see your pod will be going in pending state 


kubectl delete -f node-aff-req.yaml




Lab 7: Namespaces and Resource Quotas in Kubernetes

Task 1: Creating a Namespace
kubectl create namespace devops
kubectl get namespaces 


Create Limit range 
vi limit-range.yaml

apiVersion: v1
kind: LimitRange
metadata:
  name: devops-lr
  namespace: devops
spec:
   limits:
   - default: # this section defines default limits
       cpu: 500m
       memory: 512Mi
     defaultRequest: # this section defines default requests
       cpu: 100m
       memory: 256Mi
     type: Container

kubectl create -f limit-range.yaml

kubectl describe namespace devops


create following pod and see automatic resource is added to your pod 

vi simple-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: simple-pod
  namespace: devops
spec:
  containers:
  - name: pa-nginx-ctr1
    image: nginx

kubectl create -f simple-pod.yaml

kubectl describe pod simple-pod -n devops


Task 2: resource Quota overall resource allocation on devops namespace 

vi resource-quota.yaml

apiVersion: v1
kind: ResourceQuota
metadata:
  name: quota
  namespace: devops
spec:
  hard:
    requests.cpu: "1"
    requests.memory: 1Gi
    limits.cpu: "2"
    limits.memory: 2Gi
    services: "4"
    counts/pods: 10

kubectl create -f resource-quota.yaml
 kubectl describe namespace devops 



kops validate cluster
kops delete cluster <Your Cluster_Name> --yes

# Verify all the AWS resources are deleted from AWS Console, in case of any confusions, feel free to reach us. 
